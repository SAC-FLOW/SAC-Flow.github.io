<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies - Zhang et al.">
  <meta name="description" content="We introduce SAC Flow, a stable off-policy RL algorithm for flow-based policies that treats flow rollouts as sequential models and achieves state-of-the-art performance on continuous control tasks.">
  <meta name="keywords" content="reinforcement learning, flow-based policies, SAC, sequential models, robotics, continuous control, machine learning">
  <meta name="author" content="Yixian Zhang, Shu'ang Yu, Tonghe Zhang, Mo Guang, Haojia Hui, Kaiwen Long, Yu Wang, Chao Yu, Wenbo Ding">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Tsinghua University">
  <meta property="og:title" content="SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies">
  <meta property="og:description" content="We introduce SAC Flow, a stable off-policy RL algorithm for flow-based policies that treats flow rollouts as sequential models and achieves state-of-the-art performance on continuous control tasks.">
  <meta property="og:url" content="https://elessar123.github.io/SAC-Flow-Project/sac-flow">
  <meta property="og:image" content="https://elessar123.github.io/SAC-Flow-Project/static/images/sac_flow_overview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="SAC Flow Overview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Yixian Zhang">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies">
  <meta name="twitter:description" content="We introduce SAC Flow, a stable off-policy RL algorithm for flow-based policies that treats flow rollouts as sequential models and achieves state-of-the-art performance on continuous control tasks.">
  <meta name="twitter:image" content="https://elessar123.github.io/SAC-Flow-Project/static/images/sac_flow_overview.png">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling">
  <meta name="citation_author" content="Zhang, Yixian">
  <meta name="citation_author" content="Yu, Shu'ang">
  <meta name="citation_author" content="Zhang, ">
  <meta name="citation_author" content="Guang, Mo">
  <meta name="citation_author" content="Hui, Haojia">
  <meta name="citation_author" content="Long, Kaiwen">
  <meta name="citation_author" content="Wang, Yu">
  <meta name="citation_author" content="Yu, Chao">
  <meta name="citation_author" content="Ding, Wenbo">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_pdf_url" content="https://elessar123.github.io/SAC-Flow-Project/static/pdfs/sac_flow_paper.pdf">
  
  <title>SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies | Zhang et al.</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Custom Author Styles -->
  <style>
    .author-link {
      color: #3273dc;
      text-decoration: none;
      border-bottom: 1px solid transparent;
      transition: all 0.3s ease;
      cursor: pointer;
    }
    
    .author-link:hover {
      color: #2366d1;
      border-bottom-color: #2366d1;
      text-decoration: none;
    }
    
    .author-no-link {
      color: #4a4a4a;
      cursor: default;
      position: relative;
    }
    
    .author-no-link::after {
      content: "📧";
      font-size: 0.7em;
      margin-left: 2px;
      opacity: 0.6;
      vertical-align: super;
    }
    
    @media (max-width: 768px) {
      .author-no-link::after {
        display: none;
      }
    }
  </style>
  
  <!-- JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling",
    "description": "We introduce SAC Flow, a stable off-policy RL algorithm for flow-based policies that treats flow rollouts as sequential models and achieves state-of-the-art performance on continuous control tasks.",
    "author": [
      {
        "@type": "Person",
        "name": "Yixian Zhang",
        "affiliation": { "@type": "Organization", "name": "Tsinghua University" }
      },
      {
        "@type": "Person",
        "name": "Shu'ang Yu", 
        "affiliation": { "@type": "Organization", "name": "Tsinghua University" }
      },
      {
        "@type": "Person",
        "name": "Chao Yu",
        "affiliation": { "@type": "Organization", "name": "Tsinghua University" }
      }
    ],
    "datePublished": "2024-01-01",
    "keywords": ["reinforcement learning", "flow-based policies", "SAC", "sequential models", "robotics"],
    "abstract": "Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: Flow-G, which incorporates a gated velocity, and Flow-T, which utilizes a decoded velocity.",
    "isAccessibleForFree": true
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://elessar123.github.io/yixian_homepage.github.io/" target="_blank" class="author-link">Yixian Zhang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=wb79fkMAAAAJ&hl=en" target="_blank" class="author-link">Shu'ang Yu</a><sup>1,5*</sup>,</span>
              <span class="author-block">
                <a href="https://tonghe-zhang.github.io/" target="_blank" class="author-link">Tonghe Zhang</a><sup>2</sup>,</span>
              <span class="author-no-link">Mo Guang<sup>3</sup>,</span>
              <span class="author-no-link">Haojia Hui<sup>3</sup>,</span>
              <span class="author-no-link">Kaiwen Long<sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://nicsefc.ee.tsinghua.edu.cn/" target="_blank" class="author-link">Yu Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://nicsefc.ee.tsinghua.edu.cn/people/ChaoYu" target="_blank" class="author-link">Chao Yu</a><sup>1,4†</sup>,</span>
              <span class="author-block">
                <a href="https://ssr-group.net/index.html" target="_blank" class="author-link">Wenbo Ding</a><sup>1†</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Tsinghua University</span>
              <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
              <span class="author-block"><sup>3</sup>Li Auto</span><br>
              <span class="author-block"><sup>4</sup>Zhongguancun Academy</span>
              <span class="author-block"><sup>5</sup>Shanghai AI Laboratory</span><br>
              <span class="eql-cntrb"><small><sup>*</sup>Equal contribution <sup>†</sup>Corresponding Authors</small></span>
            </div>

            <!-- <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2509.25756" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

              <span class="link-block">
                <a href="https://github.com/Elessar123/SAC-FLOW" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://arxiv.org/abs/2509.25756" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
        </div>
      </div>
    </div>
  </div>
</div>
</div>
</section>

<!-- Overview Figure -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview.png" alt="SAC Flow Overview" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        <strong>Overview of SAC Flow.</strong> The multi-step sampling process of flow-based policies frequently causes exploding gradients during off-policy RL updates. Our key insight is to treat the flow-based policy as a sequential model, for which we first demonstrate an algebraic equivalence to an RNN. We then reparameterize the flow's velocity network using modern sequential architectures (e.g., GRU, Transformer).
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Training expressive flow-based policies with off-policy reinforcement learning is notoriously unstable due to gradient pathologies in the multi-step action sampling process. We trace this instability to a fundamental connection: the flow rollout is algebraically equivalent to a residual recurrent computation, making it susceptible to the same vanishing and exploding gradients as RNNs. To address this, we reparameterize the velocity network using principles from modern sequential models, introducing two stable architectures: <strong>Flow-G</strong>, which incorporates a gated velocity, and <strong>Flow-T</strong>, which utilizes a decoded velocity. We then develop a practical SAC-based algorithm, enabled by a noise-augmented rollout, that facilitates direct end-to-end training of these policies. Our approach supports both from-scratch and offline-to-online learning and achieves state-of-the-art performance on continuous control and robotic manipulation benchmarks, eliminating the need for common workarounds like policy distillation or surrogate objectives.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Method</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            Flow-based policies have shown strong potential on challenging continuous-control tasks, including robot manipulation, due to their ability to represent rich, multimodal action distributions. However, training these policies with off-policy reinforcement learning is notoriously unstable.
          </p>
          <p>
            Our key insight is to treat the flow-based policy as a <strong>sequential model</strong>. We show that the Euler integration used to generate actions in the flow-based policy is algebraically identical to the recurrent computation of a residual RNN, shown in (a). This observation explains the instability observed with off-policy training: the same vanishing or exploding gradients known to affect RNNs also afflict the flow rollout.
          </p>
        </div>
      </div>
    </div>
    
    <!-- Method Architecture Images -->
    <div class="columns is-centered">
      <div class="column">
        <img src="static/images/method_architectures.png" alt="Flow-based Policy Architectures" style="width: 100%; height: auto;">
        <p class="has-text-centered"><strong>Velocity network parameterizations.</strong> (a) Standard flow as RNN (b) Flow-G with GRU-style gating (c) Flow-T with Transformer decoder</p>
            <div class="content has-text-justified" style="margin-top: 1.5rem;">
        <p>
            Introducing a gate network leads to Flow-G in (b) and improves gradient stability. Replacing the velocity with the normalized residual block in (v) yields Flow-T. This architecture provides well-conditioned depth and, crucially, aggregates context with the well-established Transformer architectures. These parameterizations serve as drop-in replacements for velocity in flow policy without altering the surrounding algorithm. As a result, they enable direct and stable off-policy training with methods such as SAC, remove the need for auxiliary distillation actors and surrogate objectives, and keep flow rollout efficient at test time.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>
    <h3 class="title is-4">Experimental Domain</h3>
      <div class="columns is-centered">
        <div class="column">
          <img src="static/images/exp_env.png" alt="Exp_envs" style="width: 100%; height: auto;">
        </div>
      </div>
    
    <!-- From-scratch Results -->
    <div class="content">
      <h3 class="title is-4">From-Scratch Training Performance</h3>
      <p class="has-text-justified">
        We evaluate our method on MuJoCo continuous control tasks for from-scratch learning. Our SAC Flow-G and SAC Flow-T consistently outperform all baselines across all tasks, achieving significant sample efficiency and convergence stability.
      </p>
      
      <div class="columns is-centered">
        <div class="column">
          <img src="static/images/from_scratch_results.png" alt="From-scratch training results" style="width: 100%; height: auto;">
          <p class="has-text-centered"><strong>From-scratch training performance</strong> on MuJoCo tasks. Our methods achieve state-of-the-art performance across all environments. Meanwhile, in the sparse reward environment, all methods fail, where the offline-to-online training is essential.</p>
        </div>
      </div>
    </div>

    <!-- Offline-to-Online Results -->
    <div class="content" style="margin-top: 3rem;">
      <h3 class="title is-4">Offline-to-Online Training Performance</h3>
      <p class="has-text-justified">
        For challenging sparse-reward tasks, we evaluate on OGBench and Robomimic benchmarks using offline-to-online training. Our methods achieve rapid convergence and state-of-the-art success rates, particularly excelling in complex manipulation tasks.
      </p>
      
      <div class="columns is-centered">
        <div class="column">
          <img src="static/images/offline_to_online_results.png" alt="Offline-to-online training results" style="width: 100%; height: auto;">
          <p class="has-text-centered"><strong>Offline-to-online performance</strong> aggregated across OGBench and Robomimic tasks. Each curve shows mean success rate across multiple task instances.</p>
        </div>
      </div>
    </div>

    <!-- Ablation Studies -->
    <div class="content" style="margin-top: 3rem;">
      <h3 class="title is-4">Ablation Studies</h3>
      
      <div class="columns">
        <div class="column">
          <img src="static/images/gradient_norm_ablation.png" alt="Gradient norm ablation" style="width: 100%; height: auto;">
          <p class="has-text-centered"><strong>Gradient Stability.</strong> Our methods significantly reduce gradient exploding compared to naive SAC Flow.</p>
        </div>
        <!-- <div class="column">
          <img src="static/images/sampling_steps_ablation.png" alt="Sampling steps ablation" style="width: 100%; height: auto;">
          <p class="has-text-centered"><strong>Robustness to Sampling Steps.</strong> Our approach is robust to different numbers of flow sampling steps.</p>
        </div> -->
      </div>
    </div>
  </div>
</section>

<!-- Key Contributions -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Key Contributions</h2>
    <div class="content">
      <div class="columns">
        <div class="column">
          <div class="box">
            <h4 class="title is-5">🧠 Sequential Model Perspective</h4>
            <p>We formalize the K-step flow rollout as a residual RNN computation, providing theoretical explanation for gradient pathologies and enabling reparameterization with modern sequential architectures.</p>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h4 class="title is-5">⚡ Practical SAC Framework</h4>
            <p>We develop SAC Flow, a robust off-policy algorithm with noise-augmented rollout for tractable likelihood computation, supporting both from-scratch and offline-to-online training.</p>
          </div>
        </div>
        <div class="column">
          <div class="box">
            <h4 class="title is-5">🏆 State-of-the-Art Performance</h4>
            <p>Our methods achieve superior sample efficiency and performance across MuJoCo, OGBench, and Robomimic benchmarks, significantly outperforming recent flow- and diffusion-based baselines.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">BibTeX</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
    <pre id="bibtex-code"><code>@article{sacflow,
      title={SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling}, 
      author={Yixian Zhang and Shu'ang Yu and Tonghe Zhang and Mo Guang and Haojia Hui and Kaiwen Long and Yu Wang and Chao Yu and Wenbo Ding},
      year={2025},
      eprint={2509.25756},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2509.25756}, 
}</code></pre>
  </div>
</section>

<!-- <footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. 
          This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</div>
</footer> -->

<script>
function scrollToTop() {
  window.scrollTo({top: 0, behavior: 'smooth'});
}

function copyBibTeX() {
  const bibtexText = document.getElementById('bibtex-code').textContent;
  navigator.clipboard.writeText(bibtexText).then(function() {
    const copyBtn = document.querySelector('.copy-bibtex-btn .copy-text');
    copyBtn.textContent = 'Copied!';
    setTimeout(() => {
      copyBtn.textContent = 'Copy';
    }, 2000);
  });
}

// Show scroll to top button when scrolling
window.onscroll = function() {
  const scrollBtn = document.querySelector('.scroll-to-top');
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    scrollBtn.style.display = "block";
  } else {
    scrollBtn.style.display = "none";
  }
};
</script>

</body>
</html>
